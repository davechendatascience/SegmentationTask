{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panoptic Segmentation with CLIP + Mask2Former (LoRA)\n",
    "\n",
    "This notebook implements a panoptic segmentation pipeline using a CLIP backbone (fine-tuned with LoRA) and a lightweight Mask2Former-style decoder.\n",
    "\n",
    "**Note**: The dataset is downloaded manually from MIT SceneParsing to ensure stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install transformers datasets albumentations peft torchmetrics scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from datasets import load_dataset # Eliminated\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import CLIPVisionModel, CLIPConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
    "NUM_CLASSES = 150 \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Dataset Implementation ---\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "# --- Configuration Block ---\n",
    "# Adjust these based on your specific Colab environment and requirements\n",
    "DATASET_NAME = \"scene_parse_150\" # HuggingFace dataset name for ADE20k\n",
    "IMAGE_SIZE = 512  # Resize images to this dimension (Square)\n",
    "CLIP_MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "class ADE20kPanopticDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for ADE20k Panoptic Segmentation.\n",
    "    \n",
    "    Note: The official HuggingFace 'scene_parse_150' dataset provides semantic and instance masks.\n",
    "    For true 'panoptic' format, we typically combine these.\n",
    "    However, for this simplified implementation, we will treat it as a collection of binary masks \n",
    "    and class labels, which is what Mask2Former expects.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir=\"./ADEChallengeData2016\", split=\"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = \"training\" if split == \"train\" else \"validation\"\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Check if dataset exists, if not download\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            self.download_ade20k()\n",
    "            \n",
    "        self.image_dir = os.path.join(self.root_dir, \"images\", self.split)\n",
    "        self.mask_dir = os.path.join(self.root_dir, \"annotations\", self.split)\n",
    "        \n",
    "        self.images = sorted(glob.glob(os.path.join(self.image_dir, \"*.jpg\")))\n",
    "        self.masks = sorted(glob.glob(os.path.join(self.mask_dir, \"*.png\")))\n",
    "        \n",
    "        print(f\"Found {len(self.images)} images in {self.image_dir}\")\n",
    "\n",
    "    def download_ade20k(self):\n",
    "        print(\"Downloading ADE20k dataset (this may take a while)...\")\n",
    "        # Direct link to ADE20k\n",
    "        url = \"http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\"\n",
    "        zip_path = \"ADEChallengeData2016.zip\"\n",
    "        \n",
    "        if not os.path.exists(zip_path):\n",
    "            os.system(f\"wget {url} -O {zip_path}\")\n",
    "            \n",
    "        print(\"Unzipping...\")\n",
    "        os.system(f\"unzip -q {zip_path}\")\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Annotation in ADE20k zip: Int masks\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        # Convert back to numpy if albumentations converted to tensor\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.numpy()\n",
    "\n",
    "        # Mask2Former expects:\n",
    "        # - pixel_values: (C, H, W) -> Normalized image\n",
    "        # - pixel_mask: (H, W) -> Padding mask (optional)\n",
    "        # - mask_labels: list of binary masks (N, H, W)\n",
    "        # - class_labels: list of class ids (N)\n",
    "        \n",
    "        # Process Mask into Binary Masks + Labels\n",
    "        unique_ids = np.unique(mask)\n",
    "        # Remove background/ignore index if present (usually 0 or 255)\n",
    "        unique_ids = unique_ids[unique_ids != 0] \n",
    "        \n",
    "        masks = []\n",
    "        labels = []\n",
    "        \n",
    "        for uid in unique_ids:\n",
    "            # Create binary mask for this instance/class\n",
    "            binary_mask = (mask == uid).astype(np.float32)\n",
    "            masks.append(binary_mask)\n",
    "            labels.append(uid - 1) # ADE20k IDs are 1-150. We need 0-149 for model.\n",
    "            \n",
    "        if len(masks) > 0:\n",
    "            masks = torch.tensor(np.stack(masks), dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            # Handle standard case with no objects (rare in ADE20k)\n",
    "            masks = torch.zeros((0, IMAGE_SIZE, IMAGE_SIZE), dtype=torch.float32)\n",
    "            labels = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "        # Normalize image for CLIP\n",
    "        # CLIP Expects:\n",
    "        # mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        # std  = [0.26862954, 0.26130258, 0.27577711]\n",
    "        # Validated against CLIPProcessor defaults\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image, \n",
    "            \"masks\": masks, \n",
    "            \"class_labels\": labels,\n",
    "            \"original_size\": (image.shape[1], image.shape[2]) # H, W after transform (or keep original before)\n",
    "        }\n",
    "\n",
    "def get_transforms(image_size=512):\n",
    "    # CLIP Normalization constants\n",
    "    mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "    std  = (0.26862954, 0.26130258, 0.27577711)\n",
    "    \n",
    "    return A.Compose([\n",
    "        A.Resize(height=image_size, width=image_size),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def collafe_fn(batch):\n",
    "    # Custom collate because masks have variable channel (N instances)\n",
    "    pixel_values = torch.stack([x['pixel_values'] for x in batch])\n",
    "    \n",
    "    targets = []\n",
    "    for x in batch:\n",
    "        targets.append({\n",
    "            \"masks\": x['masks'],\n",
    "            \"class_labels\": x['class_labels']\n",
    "        })\n",
    "        \n",
    "    return pixel_values, targets\n",
    "\n",
    "# --- Usage Example ---\n",
    "# dataset = ADE20kPanopticDataset(split=\"train\", transform=get_transforms(IMAGE_SIZE))\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collafe_fn)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Model Architecture ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPVisionModel, CLIPConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class CLIPBackbone(nn.Module):\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch16\", use_lora=True, lora_rank=16):\n",
    "        super().__init__()\n",
    "        # Load standard CLIP Vision Model\n",
    "        # SPEED OPTIMIZATION: Default output_hidden_states=False is much faster\n",
    "        self.base_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        \n",
    "        if use_lora:\n",
    "            print(f\"Injecting LoRA adapters with rank={lora_rank}...\")\n",
    "            peft_config = LoraConfig(\n",
    "                r=lora_rank, \n",
    "                lora_alpha=lora_rank*2, \n",
    "                target_modules=[\"q_proj\", \"v_proj\"], \n",
    "                lora_dropout=0.1, \n",
    "                bias=\"none\",\n",
    "                modules_to_save=[], \n",
    "            )\n",
    "            self.base_model = get_peft_model(self.base_model, peft_config)\n",
    "            self.base_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        outputs = self.base_model(pixel_values=x, interpolate_pos_encoding=True)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        patch_tokens = last_hidden[:, 1:, :] \n",
    "        B, L, D = patch_tokens.shape\n",
    "        H = W = int(L**0.5) \n",
    "        \n",
    "        feature_map = patch_tokens.permute(0, 2, 1).reshape(B, D, H, W)\n",
    "                    \n",
    "        return feature_map\n",
    "\n",
    "class SimpleFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    ViTDet-style Simple Feature Pyramid.\n",
    "    Builds a pyramid from a single high-level feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.simfpn0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, hidden_dim, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.simfpn1 = nn.Sequential(\n",
    "             nn.ConvTranspose2d(in_channels, hidden_dim, kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.simfpn2 = nn.Sequential(\n",
    "            nn.Identity(), \n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "        )\n",
    "        self.simfpn3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.simfpn3_proj = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 768, H/16, W/16]\n",
    "        p2 = self.simfpn0(x) # 1/4\n",
    "        p3 = self.simfpn1(x) # 1/8\n",
    "        p4 = self.simfpn2(x) # 1/16\n",
    "        p5 = self.simfpn3(x) # 1/32\n",
    "        p5 = self.simfpn3_proj(p5)\n",
    "        \n",
    "        return [p2, p3, p4, p5] \n",
    "\n",
    "class LightMask2Former(nn.Module):\n",
    "    def __init__(self, in_channels=256, num_queries=100, num_classes=150, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_queries = num_queries\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, dim_feedforward=1024)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "        \n",
    "        self.class_head = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.mask_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # P3 and P4 projection layers? \n",
    "        # Actually we assume FPN outputs are all 'hidden_dim' channels, so we can reuse logic.\n",
    "        \n",
    "    def forward_mask_prediction(self, mask_embed, feature_map):\n",
    "        # mask_embed: [B, Q, C]\n",
    "        # feature_map: [B, C, H, W]\n",
    "        B, C, H, W = feature_map.shape\n",
    "        pixel_embed_flat = feature_map.flatten(2) # [B, C, HW]\n",
    "        pred_masks = torch.bmm(mask_embed, pixel_embed_flat)\n",
    "        pred_masks = pred_masks.reshape(B, self.num_queries, H, W)\n",
    "        return pred_masks\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [P2, P3, P4, P5]\n",
    "        # Main Scale: P2\n",
    "        pixel_embed = features[0] \n",
    "        B, C, H, W = pixel_embed.shape\n",
    "        \n",
    "        # Flatten [H*W, B, C]\n",
    "        pixel_embed_flat = pixel_embed.flatten(2).permute(2, 0, 1)\n",
    "        \n",
    "        queries = self.query_embed.weight.unsqueeze(1).repeat(1, B, 1) # [Q, B, C]\n",
    "        \n",
    "        # Decode\n",
    "        out_queries = self.transformer_decoder(tgt=queries, memory=pixel_embed_flat)\n",
    "        out_queries = out_queries.permute(1, 0, 2) # [B, Q, C]\n",
    "        \n",
    "        pred_logits = self.class_head(out_queries)\n",
    "        mask_embed = self.mask_head(out_queries) # [B, Q, C]\n",
    "        \n",
    "        # Main Prediction (P2)\n",
    "        pred_masks = self.forward_mask_prediction(mask_embed, features[0])\n",
    "        \n",
    "        # Auxiliary Predictions (P3, P4) for Consistency Loss\n",
    "        # We use the SAME mask embeddings, just projected onto coarser feature maps.\n",
    "        # This forces the feature maps to be consistent.\n",
    "        pred_masks_p3 = self.forward_mask_prediction(mask_embed, features[1])\n",
    "        pred_masks_p4 = self.forward_mask_prediction(mask_embed, features[2])\n",
    "        \n",
    "        return {\n",
    "            \"pred_logits\": pred_logits,\n",
    "            \"pred_masks\": pred_masks,\n",
    "            \"aux_outputs\": [\n",
    "                {\"pred_masks\": pred_masks_p3}, # Scale 1/8\n",
    "                {\"pred_masks\": pred_masks_p4}  # Scale 1/16\n",
    "            ]\n",
    "        }\n",
    "\n",
    "class CLIPPanopticModel(nn.Module):\n",
    "    def __init__(self, num_classes=150, lora_rank=64):\n",
    "        super().__init__()\n",
    "        self.backbone = CLIPBackbone(lora_rank=lora_rank)\n",
    "        self.pixel_decoder = SimpleFPN(in_channels=768, hidden_dim=256)\n",
    "        self.decoder = LightMask2Former(in_channels=256, hidden_dim=256, num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        backbone_feature = self.backbone(x)\n",
    "        fpn_features = self.pixel_decoder(backbone_feature)\n",
    "        outputs = self.decoder(fpn_features)\n",
    "        return outputs\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Evaluation & Visualization ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    \n",
    "    print(\"Running evaluation...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        pixel_values, targets = batch\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        \n",
    "        # Function to ensure targets are in format expected by torchmetrics\n",
    "        # Torchmetrics expects:\n",
    "        # - targets: list of dicts with 'masks' (bool or uint8), 'labels', 'boxes' (optional for segm but good to have)\n",
    "        formatted_targets = []\n",
    "        for t in targets:\n",
    "             # Masks: [N, H, W] -> Boolean\n",
    "            masks_bool = t['masks'].to(device) > 0.5\n",
    "            formatted_targets.append({\n",
    "                \"masks\": masks_bool,\n",
    "                \"labels\": t['class_labels'].to(device)\n",
    "            })\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "        # Process Outputs\n",
    "        # pred_logits: [B, Q, K+1]\n",
    "        # pred_masks: [B, Q, H, W]\n",
    "        preds = []\n",
    "        for i in range(len(formatted_targets)):\n",
    "            logits = outputs['pred_logits'][i]\n",
    "            masks_logits = outputs['pred_masks'][i]\n",
    "            \n",
    "            # Probabilities and labels\n",
    "            prob = logits.softmax(-1) # [Q, K+1]\n",
    "            scores, labels = prob[:, :-1].max(-1) # Exclude 'no-object' class\n",
    "            \n",
    "            # Filter low confidence\n",
    "            # DETR models have low confidence initially. \n",
    "            # 0.5 is too high for early epochs and kills mAP (cuts off PR curve).\n",
    "            keep = scores > 0.05 \n",
    "            \n",
    "            if keep.sum() == 0:\n",
    "                # No predictions\n",
    "                preds.append({\n",
    "                    \"masks\": torch.zeros((0, *masks_logits.shape[-2:]), dtype=torch.bool, device=device),\n",
    "                    \"scores\": torch.tensor([], device=device),\n",
    "                    \"labels\": torch.tensor([], device=device)\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            filtered_scores = scores[keep]\n",
    "            filtered_labels = labels[keep]\n",
    "            filtered_masks = masks_logits[keep]\n",
    "            \n",
    "            # Upsample masks to target resolution (if needed, usually done by metric but let's match target)\n",
    "            # Assuming target resolution is 512x512 (same as input)\n",
    "            # Model outputs low-res or 512x512 depending on decoder upsample. \n",
    "            # Our LightMask2Former outputs 16x downsampled or similar if not upsampled at end.\n",
    "            # Let's force upsample to IMAGE_SIZE (512)\n",
    "            target_H, target_W = formatted_targets[i]['masks'].shape[-2:]\n",
    "            \n",
    "            filtered_masks = F.interpolate(filtered_masks.unsqueeze(1), size=(target_H, target_W), mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "            filtered_masks = filtered_masks.sigmoid() > 0.5\n",
    "            \n",
    "            preds.append({\n",
    "                \"masks\": filtered_masks,\n",
    "                \"scores\": filtered_scores,\n",
    "                \"labels\": filtered_labels\n",
    "            })\n",
    "            \n",
    "        metric.update(preds, formatted_targets)\n",
    "        \n",
    "    result = metric.compute()\n",
    "    return result\n",
    "\n",
    "def visualize_prediction(model, dataset, idx, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load raw item for display\n",
    "    # We need transforms for model input, but want raw for display\n",
    "    # Re-access dataset item\n",
    "    item_dict = dataset[idx]\n",
    "    image_tensor = item_dict['pixel_values'].unsqueeze(0).to(device) # [1, 3, H, W]\n",
    "    \n",
    "    # Ground Truth\n",
    "    gt_masks = item_dict['masks']\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "    \n",
    "    # Decode Prediction\n",
    "    logits = outputs['pred_logits'][0]\n",
    "    pred_masks = outputs['pred_masks'][0]\n",
    "    \n",
    "    prob = logits.softmax(-1)\n",
    "    scores, labels = prob[:, :-1].max(-1)\n",
    "    \n",
    "    keep = scores > 0.05\n",
    "    final_masks = pred_masks[keep]\n",
    "    final_scores = scores[keep]\n",
    "    final_labels = labels[keep]\n",
    "    \n",
    "    # Upsample\n",
    "    if len(final_masks) > 0:\n",
    "        H, W = image_tensor.shape[-2:]\n",
    "        final_masks = F.interpolate(final_masks.unsqueeze(1), size=(H, W), mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "        final_masks = final_masks.sigmoid() > 0.5\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    # Denormalize Image\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "    img_disp = (item_dict['pixel_values'].cpu() * std + mean).permute(1, 2, 0).numpy()\n",
    "    img_disp = np.clip(img_disp, 0, 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Original\n",
    "    axs[0].imshow(img_disp)\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # 2. Ground Truth Overlay\n",
    "    combined_gt = np.zeros_like(img_disp)\n",
    "    if len(gt_masks) > 0:\n",
    "        for i, m in enumerate(gt_masks):\n",
    "            color = np.random.rand(3)\n",
    "            # mask is float 0-1\n",
    "            m = m.numpy()\n",
    "            combined_gt[m > 0.5] = color\n",
    "    \n",
    "    axs[1].imshow(img_disp)\n",
    "    axs[1].imshow(combined_gt, alpha=0.5)\n",
    "    axs[1].set_title(\"Ground Truth\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # 3. Prediction Overlay\n",
    "    combined_pred = np.zeros_like(img_disp)\n",
    "    if len(final_masks) > 0:\n",
    "        for i, m in enumerate(final_masks):\n",
    "            color = np.random.rand(3)\n",
    "            m = m.cpu().numpy()\n",
    "            combined_pred[m > 0.5] = color\n",
    "            \n",
    "    axs[2].imshow(img_disp)\n",
    "    axs[2].imshow(combined_pred, alpha=0.5)\n",
    "    axs[2].set_title(f\"Prediction ({len(final_masks)} objects)\")\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Loss & Matcher ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "\n",
    "# ADE20k Class Names (150 classes)\n",
    "ADE20K_CLASSES = [\n",
    "    \"wall\", \"building\", \"sky\", \"floor\", \"tree\", \"ceiling\", \"road\", \"bed\", \"windowpane\", \"grass\", \"cabinet\",\n",
    "    \"sidewalk\", \"person\", \"earth\", \"door\", \"table\", \"mountain\", \"plant\", \"curtain\", \"chair\", \"car\",\n",
    "    \"water\", \"painting\", \"sofa\", \"shelf\", \"house\", \"sea\", \"mirror\", \"rug\", \"field\", \"armchair\", \"seat\",\n",
    "    \"fence\", \"desk\", \"rock\", \"wardrobe\", \"lamp\", \"bathtub\", \"railing\", \"cushion\", \"base\", \"box\", \"column\",\n",
    "    \"signboard\", \"chest of drawers\", \"counter\", \"sand\", \"sink\", \"skyscraper\", \"fireplace\", \"refrigerator\",\n",
    "    \"grandstand\", \"path\", \"stairs\", \"runway\", \"case\", \"pool table\", \"pillow\", \"screen door\", \"stairway\",\n",
    "    \"river\", \"bridge\", \"bookcase\", \"blind\", \"coffee table\", \"toilet\", \"flower\", \"book\", \"hill\", \"bench\",\n",
    "    \"countertop\", \"stove\", \"palm\", \"kitchen island\", \"computer\", \"swivel chair\", \"boat\", \"bar\", \"arcade machine\",\n",
    "    \"hovel\", \"bus\", \"towel\", \"light\", \"truck\", \"tower\", \"chandelier\", \"awning\", \"streetlight\", \"booth\",\n",
    "    \"television receiver\", \"airplane\", \"dirt track\", \"apparel\", \"pole\", \"land\", \"bannister\", \"escalator\",\n",
    "    \"ottoman\", \"bottle\", \"buffet\", \"poster\", \"stage\", \"van\", \"ship\", \"fountain\", \"conveyer belt\", \"canopy\",\n",
    "    \"washer\", \"plaything\", \"swimming pool\", \"stool\", \"barrel\", \"basket\", \"waterfall\", \"tent\", \"bag\", \"minibike\",\n",
    "    \"cradle\", \"oven\", \"ball\", \"food\", \"step\", \"tank\", \"trade name\", \"microwave\", \"pot\", \"animal\", \"bicycle\",\n",
    "    \"lake\", \"dishwasher\", \"screen\", \"blanket\", \"sculpture\", \"hood\", \"sconce\", \"vase\", \"traffic light\", \"tray\",\n",
    "    \"ashcan\", \"fan\", \"pier\", \"crt screen\", \"plate\", \"monitor\", \"bulletin board\", \"shower\", \"radiator\",\n",
    "    \"glass\", \"clock\", \"flag\"\n",
    "]\n",
    "\n",
    "class BoundaryAwareLoss(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super().__init__()\n",
    "        kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=torch.float32)\n",
    "        self.kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def forward(self, pred_masks, target_masks):\n",
    "        if pred_masks.numel() == 0:\n",
    "            return pred_masks.sum() * 0\n",
    "        pred_input = pred_masks.unsqueeze(1) \n",
    "        target_input = target_masks.unsqueeze(1) \n",
    "        kernel = self.kernel.to(pred_masks.device)\n",
    "        pred_edges = F.conv2d(pred_input, kernel, padding=1)\n",
    "        target_edges = F.conv2d(target_input, kernel, padding=1)\n",
    "        return F.l1_loss(pred_edges, target_edges)\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, cost_class=1, cost_mask=1, cost_dice=1):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_mask = cost_mask\n",
    "        self.cost_dice = cost_dice\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  \n",
    "        out_mask = outputs[\"pred_masks\"].flatten(0, 1).flatten(1)  \n",
    "        tgt_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
    "        tgt_mask = torch.cat([v[\"masks\"] for v in targets])\n",
    "        H_p, W_p = outputs[\"pred_masks\"].shape[-2:]\n",
    "        tgt_mask = F.interpolate(tgt_mask.unsqueeze(1), size=(H_p, W_p), mode='nearest').squeeze(1)\n",
    "        tgt_mask = tgt_mask.flatten(1) \n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "        cost_mask = torch.cdist(out_mask, tgt_mask, p=1)\n",
    "        out_mask_sig = out_mask.sigmoid()\n",
    "        numerator = 2 * torch.mm(out_mask_sig, tgt_mask.t())\n",
    "        denominator = out_mask_sig.sum(-1).unsqueeze(1) + tgt_mask.sum(-1).unsqueeze(0)\n",
    "        cost_dice = 1 - (numerator / (denominator + 1e-6))\n",
    "        C = self.cost_class * cost_class + self.cost_mask * cost_mask + self.cost_dice * cost_dice\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "        indices = []\n",
    "        sizes = [len(v[\"class_labels\"]) for v in targets]\n",
    "        for i, c in enumerate(C.split(sizes, -1)):\n",
    "            if c.shape[-1] == 0: \n",
    "                indices.append((torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)))\n",
    "                continue\n",
    "            row_ind, col_ind = linear_sum_assignment(c[0]) \n",
    "            indices.append((torch.as_tensor(row_ind, dtype=torch.int64), torch.as_tensor(col_ind, dtype=torch.int64)))\n",
    "        return indices\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, matcher, weight_dict, num_parents=30, label_smoothing=0.1, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.boundary_loss_func = BoundaryAwareLoss()\n",
    "        \n",
    "        # Hierarchical Config\n",
    "        self.num_parents = num_parents\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.hierarchy = None\n",
    "        self.device_name = device # Store device string\n",
    "        \n",
    "        # To be computed on first forward or init\n",
    "        # We compute it lazily or here if device is ready\n",
    "        self._hierarchy_computed = False\n",
    "\n",
    "    def _compute_hierarchy(self, device):\n",
    "        print(\"Computing Class Hierarchy from CLIP Embeddings...\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        \n",
    "        class_names = ADE20K_CLASSES[:self.num_classes] \n",
    "        inputs = tokenizer(class_names, padding=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = model.get_text_features(**inputs)\n",
    "            \n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        # Cosine distance\n",
    "        norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / (norm + 1e-8)\n",
    "        distances = 1 - (embeddings @ embeddings.T)\n",
    "        distances = (distances + distances.T) / 2 # Enforce symmetry\n",
    "        np.fill_diagonal(distances, 0) # Enforce 0 diagonal for scipy\n",
    "        distances = distances.clip(min=0)\n",
    "        \n",
    "        # Clustering\n",
    "        condensed = squareform(distances) # Clip negative precision errors\n",
    "        linkage_matrix = linkage(condensed, method='ward')\n",
    "        cluster_labels = fcluster(linkage_matrix, self.num_parents, criterion='maxclust')\n",
    "        \n",
    "        self.hierarchy = torch.tensor(cluster_labels - 1, dtype=torch.long, device=device)\n",
    "        self._hierarchy_computed = True\n",
    "        print(\"Hierarchy Computed.\")\n",
    "\n",
    "    def create_soft_target(self, target_classes_o, device):\n",
    "        \"\"\"\n",
    "        Create soft target for matched queries (B_matches, NumClasses)\n",
    "        \"\"\"\n",
    "        N = target_classes_o.shape[0]\n",
    "        soft_target = torch.ones(N, self.num_classes, device=device) * (self.label_smoothing / (self.num_classes - 1))\n",
    "        \n",
    "        for i, class_id in enumerate(target_classes_o):\n",
    "            parent_id = self.hierarchy[class_id]\n",
    "            \n",
    "            # True class\n",
    "            soft_target[i, class_id] = 1.0 - self.label_smoothing\n",
    "            \n",
    "            # Siblings\n",
    "            sibling_mask = (self.hierarchy == parent_id)\n",
    "            sibling_ids = torch.where(sibling_mask)[0]\n",
    "            \n",
    "            if len(sibling_ids) > 1:\n",
    "                remaining_mass = self.label_smoothing / len(sibling_ids)\n",
    "                # Distribute to siblings (including self, but self already boosted, so effectively boost 'other' siblings)\n",
    "                # Actually user logic: \"Distribute remaining mass among siblings\"\n",
    "                # Let's simple add boosted probability to siblings\n",
    "                soft_target[i, sibling_ids] += remaining_mass\n",
    "\n",
    "        # Renormalize\n",
    "        soft_target = soft_target / (soft_target.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        return soft_target\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        src_logits = outputs['pred_logits'] # [B, Q, K+1]\n",
    "        src_logits = src_logits[..., :-1]   # [B, Q, K]\n",
    "        \n",
    "        idx = self._get_src_permutation_idx(indices) # (Batch_idx, Query_idx) for Matches\n",
    "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        \n",
    "        # Initialize Hierarchy if needed\n",
    "        if not self._hierarchy_computed:\n",
    "            self._compute_hierarchy(src_logits.device)\n",
    "\n",
    "        # --- for \"No Object\" queries (Background), we usually push probabilities down -> entropy Max\n",
    "        # But Mask2Former typically matches specific queries.\n",
    "        # Unmatched queries (Backgound) are standard Focal Loss to 0.\n",
    "        \n",
    "        # 1. Standard Focal Loss (Classification Hard Imbalance)\n",
    "        target_classes_onehot = torch.zeros_like(src_logits)\n",
    "        target_classes_onehot[idx[0], idx[1], target_classes_o] = 1.0\n",
    "        loss_focal = sigmoid_focal_loss(src_logits, target_classes_onehot, alpha=0.25, gamma=2.0, reduction=\"sum\")\n",
    "        loss_focal = loss_focal / num_boxes\n",
    "\n",
    "        # 2. Hierarchical Loss Components (Only on MATCHED queries)\n",
    "        # We only apply semantic guidance to positive objects.\n",
    "        matched_logits = src_logits[idx] # [N_matches, K]\n",
    "        \n",
    "        if len(matched_logits) > 0:\n",
    "            # Soft Targets\n",
    "            soft_targets = self.create_soft_target(target_classes_o, src_logits.device)\n",
    "            \n",
    "            # KL Loss (Fineness)\n",
    "            # LogSoftmax on logits\n",
    "            log_probs = F.log_softmax(matched_logits, dim=1)\n",
    "            loss_kl = F.kl_div(log_probs, soft_targets, reduction='batchmean')\n",
    "            \n",
    "            # Parent Loss (Coarseness)\n",
    "            # Sum logits for parents\n",
    "            parent_logits = torch.zeros(len(matched_logits), self.num_parents, device=src_logits.device)\n",
    "            # There is probably a scatter_add_ way to do this faster, but loop is safe for now\n",
    "            for pid in range(self.num_parents):\n",
    "                child_mask = (self.hierarchy == pid)\n",
    "                if child_mask.any():\n",
    "                    # logsumexp of children logits for this parent\n",
    "                    parent_logits[:, pid] = torch.logsumexp(matched_logits[:, child_mask], dim=1)\n",
    "            \n",
    "            target_parents = self.hierarchy[target_classes_o]\n",
    "            loss_parent = F.cross_entropy(parent_logits, target_parents)\n",
    "            \n",
    "        else:\n",
    "            loss_kl = torch.tensor(0.0, device=src_logits.device)\n",
    "            loss_parent = torch.tensor(0.0, device=src_logits.device)\n",
    "\n",
    "        return {'loss_ce': loss_focal, 'loss_kl': loss_kl, 'loss_parent': loss_parent}\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        src_masks = outputs['pred_masks'][src_idx] \n",
    "        target_masks = torch.cat([t['masks'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        \n",
    "        src_masks = F.interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "\n",
    "        # Use BCEWithLogits for AMP safety\n",
    "        loss_sigmoid = F.binary_cross_entropy_with_logits(src_masks, target_masks)\n",
    "        \n",
    "        src_masks_sigmoid = src_masks.sigmoid()\n",
    "        \n",
    "        src_masks_flat = src_masks_sigmoid.flatten(1)\n",
    "        target_masks_flat = target_masks.flatten(1)\n",
    "        numerator = 2 * (src_masks_flat * target_masks_flat).sum(1)\n",
    "        denominator = src_masks_flat.sum(1) + target_masks_flat.sum(1)\n",
    "        loss_dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "        loss_dice = loss_dice.mean()\n",
    "        \n",
    "        loss_boundary = self.boundary_loss_func(src_masks_sigmoid, target_masks)\n",
    "        return {'loss_mask': loss_sigmoid, 'loss_dice': loss_dice, 'loss_boundary': loss_boundary}\n",
    "    \n",
    "    def loss_consistency(self, outputs, targets, indices, num_boxes):\n",
    "        if \"aux_outputs\" not in outputs:\n",
    "            return {'loss_consistency': torch.tensor(0.0).to(outputs['pred_logits'].device)}\n",
    "        src_masks_high = outputs['pred_masks'] \n",
    "        loss = 0.0\n",
    "        for i, aux in enumerate(outputs[\"aux_outputs\"]):\n",
    "            src_masks_low = aux[\"pred_masks\"] \n",
    "            target_size = src_masks_low.shape[-2:]\n",
    "            src_masks_high_down = F.interpolate(src_masks_high, size=target_size, mode='bilinear', align_corners=False)\n",
    "            loss += F.l1_loss(src_masks_high_down.sigmoid(), src_masks_low.sigmoid())\n",
    "        return {'loss_consistency': loss}\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        indices = self.matcher(outputs, targets)\n",
    "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        \n",
    "        losses = {}\n",
    "        losses.update(self.loss_labels(outputs, targets, indices, num_boxes))\n",
    "        losses.update(self.loss_masks(outputs, targets, indices, num_boxes))\n",
    "        losses.update(self.loss_consistency(outputs, targets, indices, num_boxes))\n",
    "        \n",
    "        # User defined weights adapted\n",
    "        w_focal = self.weight_dict.get('loss_ce', 0.25)\n",
    "        w_parent = self.weight_dict.get('loss_parent', 0.20)\n",
    "        w_kl = self.weight_dict.get('loss_kl', 0.40)\n",
    "        \n",
    "        w_mask = self.weight_dict.get('loss_mask', 5.0)\n",
    "        w_dice = self.weight_dict.get('loss_dice', 5.0) \n",
    "        w_boundary = self.weight_dict.get('loss_boundary', 2.0)\n",
    "        w_consistency = self.weight_dict.get('loss_consistency', 1.0)\n",
    "        \n",
    "        final_loss = (losses['loss_ce'] * w_focal + \n",
    "                     losses['loss_parent'] * w_parent + \n",
    "                     losses['loss_kl'] * w_kl +\n",
    "                     losses['loss_mask'] * w_mask + \n",
    "                     losses['loss_dice'] * w_dice +\n",
    "                     losses['loss_boundary'] * w_boundary +\n",
    "                     losses['loss_consistency'] * w_consistency)\n",
    "        \n",
    "        return final_loss, losses\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Training Loop Execution ---\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 1. Data\n",
    "# This will trigger download if not found\n",
    "train_ds = ADE20kPanopticDataset(split=\"train\", transform=get_transforms(IMAGE_SIZE))\n",
    "val_ds = ADE20kPanopticDataset(split=\"validation\", transform=get_transforms(IMAGE_SIZE))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collafe_fn, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collafe_fn, num_workers=2)\n",
    "\n",
    "# Configuration\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
    "NUM_CLASSES = 150 \n",
    "LORA_RANK = 16 # Adjustable LoRA Rank\n",
    "\n",
    "# 2. Model\n",
    "model = CLIPPanopticModel(num_classes=NUM_CLASSES, lora_rank=LORA_RANK)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optimization: PyTorch 2.0 Compilation\n",
    "# This fuses kernels for JAX-like performance\n",
    "if hasattr(torch, \"compile\"):\n",
    "    print(\"Compiling model with torch.compile...\")\n",
    "    model = torch.compile(model)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# 3. Loss\n",
    "matcher = HungarianMatcher()\n",
    "# Weights updated to prioritize Hierarchical Loss (Self-Supervised + Semantic)\n",
    "weight_dict = {\n",
    "    'loss_ce': 0.1,          # Reduced focal loss (let hierarchy drive)\n",
    "    'loss_parent': 1.0,      # Coarse semantic grouping\n",
    "    'loss_kl': 2.0,          # Soft target matching\n",
    "    'loss_mask': 5.0,        # Shape\n",
    "    'loss_dice': 5.0,        # Overlap\n",
    "    'loss_boundary': 2.0,    # Edges\n",
    "    'loss_consistency': 1.0  # Multi-scale\n",
    "} \n",
    "# Note: criterion will autodownload CLIP for hierarchy on init\n",
    "criterion = SetCriterion(num_classes=NUM_CLASSES, matcher=matcher, weight_dict=weight_dict, device=DEVICE).to(DEVICE)\n",
    "\n",
    "# 4. Optimizer\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad], \"lr\": 1e-5},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"decoder\" in n and p.requires_grad], \"lr\": 1e-4},\n",
    "]\n",
    "optimizer = optim.AdamW(param_dicts, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler() # Mixed Precision Scaler\n",
    "\n",
    "# 5. Loop\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    # Train One Epoch Inline\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        pixel_values, targets = batch\n",
    "        pixel_values = pixel_values.to(DEVICE)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Mixed Precision Training\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(pixel_values)\n",
    "            loss, loss_dict = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "    print(f\"Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation & Visualization Step (Every Epoch)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # Visualize Prediction on a random sample\n",
    "        print(\"Visualizing random sample...\")\n",
    "        rand_idx = np.random.randint(0, len(val_ds))\n",
    "        visualize_prediction(model, val_ds, rand_idx, DEVICE)\n",
    "\n",
    "torch.save(model.state_dict(), \"clip_panoptic_lora.pth\")\n",
    "print(\"Model saved!\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}